{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COREALGOS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFYYz3PBqAGMezhDWY64GQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhania0601/TensorFlow/blob/main/CORE_ALGOS_using%20Tensor%20Flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LINEAR REGRESSION \n"
      ],
      "metadata": {
        "id": "5cNx92Ywwyiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "L94mMEhhww5z",
        "outputId": "91d21fa8-45ef-4ab5-a931-28e63b4dc888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.74242425\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "\n",
        "import tensorflow as tf\n",
        "# Load dataset.\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "y_train = dftrain.pop('survived')\n",
        "y_eval = dfeval.pop('survived')\n",
        "\n",
        "\n",
        "dftrain.age.hist(bins=20)\n",
        "\n",
        "dftrain.sex.value_counts().plot(kind='barh')\n",
        "\n",
        "pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')\n",
        "\n",
        "\n",
        "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
        "                       'embark_town', 'alone']\n",
        "NUMERIC_COLUMNS = ['age', 'fare']\n",
        "\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column\n",
        "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
        "\n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
        "\n",
        "print(feature_columns)\n",
        "\n",
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_function():  # inner function, this will be returned\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)  # randomize order of data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
        "    return ds  # return a batch of the dataset\n",
        "  return input_function  # return a function object for use\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)\n",
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
        "linear_est.train(train_input_fn)  # train\n",
        "result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data\n",
        "\n",
        "clear_output()  # clears consoke output\n",
        "print(result['accuracy'])  # the result variable is simply a dict of stats about our model\n",
        "\n",
        "pred_dicts = list(linear_est.predict(eval_input_fn))\n",
        "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
        "\n",
        "probs.plot(kind='hist', bins=20, title='predicted probabilities')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASSIFICATION \n"
      ],
      "metadata": {
        "id": "wsZJhIF821MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "# Lets define some constants to help us later on\n",
        "\n",
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
        "\n",
        "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe\n",
        "\n",
        "train_y = train.pop('Species')\n",
        "test_y = test.pop('Species')\n",
        "train.head() # the species column is now gone\n",
        "\n",
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle and repeat if you are in training mode.\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(1000).repeat()\n",
        "    \n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "# Feature columns describe how to use the input.\n",
        "my_feature_columns = []\n",
        "for key in train.keys():\n",
        "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
        "print(my_feature_columns)\n",
        "\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=my_feature_columns,\n",
        "    # Two hidden layers of 30 and 10 nodes respectively.\n",
        "    hidden_units=[30, 10],\n",
        "    # The model must choose between 3 classes.\n",
        "    n_classes=3)\n",
        "\n",
        "classifier.train(\n",
        "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
        "    steps=5000)\n",
        "# We include a lambda to avoid creating an inner function previously\n",
        "\n",
        "eval_result = classifier.evaluate(\n",
        "    input_fn=lambda: input_fn(test, test_y, training=False))\n",
        "\n",
        "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqbpcCzf23Tu",
        "outputId": "a5109371-9494-44bd-9555-7cb566eb30d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmph5wwv9ua\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmph5wwv9ua', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmph5wwv9ua/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:loss = 1.410738, step = 0\n",
            "INFO:tensorflow:global_step/sec: 443.395\n",
            "INFO:tensorflow:loss = 0.9329598, step = 100 (0.228 sec)\n",
            "INFO:tensorflow:global_step/sec: 540.626\n",
            "INFO:tensorflow:loss = 0.8382883, step = 200 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 496.925\n",
            "INFO:tensorflow:loss = 0.7874765, step = 300 (0.200 sec)\n",
            "INFO:tensorflow:global_step/sec: 540.86\n",
            "INFO:tensorflow:loss = 0.762074, step = 400 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 543.288\n",
            "INFO:tensorflow:loss = 0.7336254, step = 500 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.19\n",
            "INFO:tensorflow:loss = 0.70665807, step = 600 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 543.044\n",
            "INFO:tensorflow:loss = 0.68594974, step = 700 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 537.534\n",
            "INFO:tensorflow:loss = 0.65924394, step = 800 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 541.442\n",
            "INFO:tensorflow:loss = 0.64110875, step = 900 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.732\n",
            "INFO:tensorflow:loss = 0.6149901, step = 1000 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 517.923\n",
            "INFO:tensorflow:loss = 0.60087854, step = 1100 (0.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 541.06\n",
            "INFO:tensorflow:loss = 0.5865778, step = 1200 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 515.523\n",
            "INFO:tensorflow:loss = 0.574721, step = 1300 (0.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 527.211\n",
            "INFO:tensorflow:loss = 0.56137025, step = 1400 (0.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 537.317\n",
            "INFO:tensorflow:loss = 0.5405976, step = 1500 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 533.716\n",
            "INFO:tensorflow:loss = 0.53221786, step = 1600 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 549.003\n",
            "INFO:tensorflow:loss = 0.5207193, step = 1700 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 549.426\n",
            "INFO:tensorflow:loss = 0.50034535, step = 1800 (0.179 sec)\n",
            "INFO:tensorflow:global_step/sec: 541.377\n",
            "INFO:tensorflow:loss = 0.4833631, step = 1900 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.006\n",
            "INFO:tensorflow:loss = 0.48210755, step = 2000 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 544.727\n",
            "INFO:tensorflow:loss = 0.46586725, step = 2100 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 539.26\n",
            "INFO:tensorflow:loss = 0.46816874, step = 2200 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.894\n",
            "INFO:tensorflow:loss = 0.44485006, step = 2300 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 537.632\n",
            "INFO:tensorflow:loss = 0.43650296, step = 2400 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 561.038\n",
            "INFO:tensorflow:loss = 0.43043816, step = 2500 (0.176 sec)\n",
            "INFO:tensorflow:global_step/sec: 542.454\n",
            "INFO:tensorflow:loss = 0.420239, step = 2600 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 553.095\n",
            "INFO:tensorflow:loss = 0.41023314, step = 2700 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 532.812\n",
            "INFO:tensorflow:loss = 0.40659225, step = 2800 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 535.711\n",
            "INFO:tensorflow:loss = 0.40138662, step = 2900 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 556.4\n",
            "INFO:tensorflow:loss = 0.3864101, step = 3000 (0.177 sec)\n",
            "INFO:tensorflow:global_step/sec: 546.244\n",
            "INFO:tensorflow:loss = 0.37054193, step = 3100 (0.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 561.761\n",
            "INFO:tensorflow:loss = 0.37748504, step = 3200 (0.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.62\n",
            "INFO:tensorflow:loss = 0.36711454, step = 3300 (0.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 529.514\n",
            "INFO:tensorflow:loss = 0.35340995, step = 3400 (0.189 sec)\n",
            "INFO:tensorflow:global_step/sec: 557.315\n",
            "INFO:tensorflow:loss = 0.35180128, step = 3500 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 551.101\n",
            "INFO:tensorflow:loss = 0.3454998, step = 3600 (0.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 543.371\n",
            "INFO:tensorflow:loss = 0.34720463, step = 3700 (0.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 554.478\n",
            "INFO:tensorflow:loss = 0.33195513, step = 3800 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 511.623\n",
            "INFO:tensorflow:loss = 0.3347565, step = 3900 (0.195 sec)\n",
            "INFO:tensorflow:global_step/sec: 545.892\n",
            "INFO:tensorflow:loss = 0.3365445, step = 4000 (0.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 542.278\n",
            "INFO:tensorflow:loss = 0.3284682, step = 4100 (0.188 sec)\n",
            "INFO:tensorflow:global_step/sec: 531.542\n",
            "INFO:tensorflow:loss = 0.31838024, step = 4200 (0.185 sec)\n",
            "INFO:tensorflow:global_step/sec: 548.923\n",
            "INFO:tensorflow:loss = 0.315409, step = 4300 (0.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 524.294\n",
            "INFO:tensorflow:loss = 0.3092805, step = 4400 (0.191 sec)\n",
            "INFO:tensorflow:global_step/sec: 534.539\n",
            "INFO:tensorflow:loss = 0.3075218, step = 4500 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 541.069\n",
            "INFO:tensorflow:loss = 0.29550463, step = 4600 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 536.738\n",
            "INFO:tensorflow:loss = 0.3037883, step = 4700 (0.186 sec)\n",
            "INFO:tensorflow:global_step/sec: 538.012\n",
            "INFO:tensorflow:loss = 0.29189128, step = 4800 (0.187 sec)\n",
            "INFO:tensorflow:global_step/sec: 545.927\n",
            "INFO:tensorflow:loss = 0.28378776, step = 4900 (0.180 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5000...\n",
            "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmph5wwv9ua/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5000...\n",
            "INFO:tensorflow:Loss for final step: 0.277529.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-12-30T06:48:36\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmph5wwv9ua/model.ckpt-5000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Inference Time : 0.23094s\n",
            "INFO:tensorflow:Finished evaluation at 2021-12-30-06:48:37\n",
            "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.96666664, average_loss = 0.3173914, global_step = 5000, loss = 0.3173914\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmph5wwv9ua/model.ckpt-5000\n",
            "\n",
            "Test set accuracy: 0.967\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(features, batch_size=256):\n",
        "    # Convert the inputs to a Dataset without labels.\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "predict = {}\n",
        "\n",
        "print(\"Please type numeric values as prompted.\")\n",
        "for feature in features:\n",
        "  valid = True\n",
        "  while valid: \n",
        "    val = input(feature + \": \")\n",
        "    if not val.isdigit(): valid = False\n",
        "\n",
        "  predict[feature] = [float(val)]\n",
        "\n",
        "predictions = classifier.predict(input_fn=lambda: input_fn(predict))\n",
        "for pred_dict in predictions:\n",
        "    class_id = pred_dict['class_ids'][0]\n",
        "    probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "        SPECIES[class_id], 100 * probability))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lANXHgwRFqGt",
        "outputId": "db3d6437-f05a-4a17-c7d4-2507915cc175"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please type numeric values as prompted.\n",
            "SepalLength: 9.6\n",
            "SepalWidth: 7.8\n",
            "PetalLength: 6.7\n",
            "PetalWidth: 4\n",
            "PetalWidth: 6\n",
            "PetalWidth: 8\n",
            "PetalWidth: 0\n",
            "PetalWidth: 3\n",
            "PetalWidth: 2\n",
            "PetalWidth: 2.4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmph5wwv9ua/model.ckpt-5000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "Prediction is \"Versicolor\" (56.3%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hidden markovs\n"
      ],
      "metadata": {
        "id": "GI__e_KlIYLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "!pip install tensorflow_probability==0.8.0rc0 --user --upgrade\n",
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow as tf\n",
        "\n",
        "tfd = tfp.distributions  # making a shortcut for later on\n",
        "initial_distribution = tfd.Categorical(probs=[0.2, 0.8])  # Refer to point 2 above\n",
        "transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n",
        "                                                 [0.2, 0.8]])  # refer to points 3 and 4 above\n",
        "observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])  # refer to point 5 above\n",
        "\n",
        "# the loc argument represents the mean and the scale is the standard devitation\n",
        "\n",
        "model = tfd.HiddenMarkovModel(\n",
        "    initial_distribution=initial_distribution,\n",
        "    transition_distribution=transition_distribution,\n",
        "    observation_distribution=observation_distribution,\n",
        "    num_steps=7)\n",
        "\n",
        "mean = model.mean()\n",
        "\n",
        "# due to the way TensorFlow works on a lower level we need to evaluate part of the graph\n",
        "# from within a session to see the value of this tensor\n",
        "\n",
        "# in the new version of tensorflow we need to use tf.compat.v1.Session() rather than just tf.Session()\n",
        "with tf.compat.v1.Session() as sess:  \n",
        "  print(mean.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYBauo6zIb34",
        "outputId": "67572a24-3d4d-4d72-b8eb-b9363ba71191"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Requirement already satisfied: tensorflow_probability==0.8.0rc0 in /root/.local/lib/python3.7/site-packages (0.8.0rc0)\n",
            "Requirement already satisfied: cloudpickle==1.1.1 in /root/.local/lib/python3.7/site-packages (from tensorflow_probability==0.8.0rc0) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow_probability==0.8.0rc0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_probability==0.8.0rc0) (1.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow_probability==0.8.0rc0) (4.4.2)\n",
            "[11.999999 10.500001  9.75      9.375     9.1875    9.09375   9.046875]\n"
          ]
        }
      ]
    }
  ]
}